{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/steve/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/steve/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_shakespeare = open('../../data/shakespeare_cleaned.txt','r')\n",
    "shakespeare = f_shakespeare.read()\n",
    "\n",
    "f_obama = open('../../data/obama_cleaned.txt','r')\n",
    "obama = f_obama.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384.4099099099099\n",
      "444\n"
     ]
    }
   ],
   "source": [
    "n = 70 #amount of words in each data point\n",
    "obama_words = obama.split(' ')\n",
    "c_obama = [\" \".join(obama_words[i:i+n]) for i in range(0, len(obama_words), n)]\n",
    "\n",
    "print(np.average([len(x) for x in c_obama]))\n",
    "print(len(c_obama))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370.10420193935664\n",
      "12994\n"
     ]
    }
   ],
   "source": [
    "n = 68 #amount of words/sentences in each data point\n",
    "shake_words = shakespeare.split(' ')\n",
    "\n",
    "#split based on n sentences - disabled for now\n",
    "#c_shakespeare = [\". \".join(shake_words[i:i+n]) for i in range(0, len(shake_words), n)]\n",
    "\n",
    "#split based on n words\n",
    "c_shakespeare = [\" \".join(shake_words[i:i+n]) for i in range(0, len(shake_words), n)]\n",
    "\n",
    "np.random.seed(59374)\n",
    "np.random.shuffle(c_shakespeare)\n",
    "\n",
    "print(np.average([len(x) for x in c_shakespeare]))\n",
    "print(len(c_shakespeare))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Most Informative Features\n",
      "                   shall = True           shakes : obama  =     35.4 : 1.0\n",
      "                  people = True            obama : shakes =     27.5 : 1.0\n",
      "                    also = True            obama : shakes =     22.3 : 1.0\n",
      "                children = True            obama : shakes =     17.8 : 1.0\n",
      "                    want = True            obama : shakes =     14.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#naive bayes - big failure but good practice\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopset = list(set(stopwords.words('english')))\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words.split() if word.lower() not in stopset])\n",
    "\n",
    "l_shakespeare = [(word_feats(x), \"shakespeare\") for x in c_shakespeare]\n",
    "l_obama = [(word_feats(x), \"obama\") for x in c_obama]\n",
    "\n",
    "total_data = l_shakespeare[:444] + l_obama\n",
    "np.random.seed(59374)\n",
    "np.random.shuffle(total_data)\n",
    "\n",
    "train_set, test_set = train_test_split(total_data)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "import operator\n",
    "import random\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles) if t not in string.punctuation and not any(char.isdigit() for char in t)]\n",
    "\n",
    "def generate_vocab(data, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Generate vocabulary from a given string\n",
    "    :param data: List of strings (should be denoised)\n",
    "    :param vocab_size: Number of entries to save\n",
    "    :return: Dictionary with words as the key and index as output\n",
    "    \"\"\"\n",
    "    word_count = {}  # count of each word encountered\n",
    "    word_index = {}  # vocabulary\n",
    "\n",
    "    for word in data.split():\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "    if vocab_size is not None:\n",
    "        ordered_words_by_index = sorted(word_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        ordered_words_by_index = ordered_words_by_index[:vocab_size]\n",
    "        for word in ordered_words_by_index:\n",
    "            word_index[word[0]] = len(word_index)\n",
    "    else:\n",
    "        for word in word_count:\n",
    "            word_index[word] = len(word_index)\n",
    "    return word_index\n",
    "\n",
    "#total_data = [(x, \"shakespeare\") for x in c_shakespeare[:444]] + [(x, \"obama\") for x in c_obama]\n",
    "q_shakespeare = [(x, \"shakespeare\") for x in c_shakespeare]\n",
    "q_obama = [(x, \"obama\") for x in c_obama]\n",
    "new_total = q_shakespeare[:444] + q_obama\n",
    "np.random.seed(59374)\n",
    "np.random.shuffle(new_total)\n",
    "\n",
    "total_dict = {\"text\" : [x[0] for x in new_total], \"author\" : [x[1] for x in new_total]}\n",
    "\n",
    "#corpus = \"\"\n",
    "#for block in total_data:\n",
    "#    corpus = corpus + block\n",
    "\n",
    "#word_sequence = text_to_word_sequence(corpus)\n",
    "#words = set(total_words)\n",
    "#vocab_size = len(words)\n",
    "#tokenizer = Tokenizer(num_words=2000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "\n",
    "#final = one_hot(corpus, round(vocab_size*1.4))\n",
    "#final = to_categorical(final)\n",
    "#print(len(words))\n",
    "\n",
    "#vocab = generate_vocab(corpus, vocab_size)\n",
    "np.random.seed(59374)\n",
    "np.random.shuffle(new_total)\n",
    "\n",
    "#vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())                         stop_words='english')\n",
    "#X = vectorizer.fit_transform(total_data)\n",
    "#print(X[:50])\n",
    "#train_set, test_set = train_test_split(total_data)\n",
    "\n",
    "#X = tokenizer.texts_to_sequences()\n",
    "#X = pad_sequences(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 author       text\n",
      "count              3513       3513\n",
      "unique                3       3479\n",
      "top     herman melville  CHAPTER .\n",
      "freq               1348         15\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def text_to_data(text, author):\n",
    "    #remove newlines, numbers, some punctuation\n",
    "    text = text.replace('\\n', \" \")\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    sent_tokenize_list = sent_tokenize(text)    \n",
    "    total_arr = [(x, author) for x in sent_tokenize_list]\n",
    "    #total_dict = {\"text\" : [x for x in sent_tokenize_list], \"author\" : [author for x in sent_tokenize_list]}\n",
    "    #total_dict = pd.DataFrame(data=total_dict)\n",
    "    vocab_count = len(set(text.split(' ')))\n",
    "    return total_arr, vocab_count\n",
    "\n",
    "def shuffle_and_convert(dictionary):\n",
    "    [[x, y] for x in dictionary['text'] for y in dictionary['author']]\n",
    "\n",
    "total_word_count = 0\n",
    "f = open('../../data/anthem.txt', 'r')\n",
    "anthem_text, count = text_to_data(f.read(), \"ayn rand\")\n",
    "total_word_count += count\n",
    "\n",
    "f = open('../../data/mobydick.txt', 'r')\n",
    "mobydick_text, count = text_to_data(f.read(), \"herman melville\")\n",
    "total_word_count += count\n",
    "\n",
    "f = open('../../data/alice.txt', 'r')\n",
    "alice_text, count = text_to_data(f.read(), 'lewis carroll')\n",
    "total_word_count += count\n",
    "\n",
    "total_arr = anthem_text + mobydick_text + alice_text\n",
    "np.random.seed(53894343)\n",
    "np.random.shuffle(total_arr)\n",
    "\n",
    "total_dict = {\"text\" : [x[0] for x in total_arr], \"author\" : [x[1] for x in total_arr]}\n",
    "total_df = pd.DataFrame(total_dict)\n",
    "print(total_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2459, 16765)\n",
      "Train on 2336 samples, validate on 123 samples\n",
      "Epoch 1/5\n",
      " 160/2336 [=>............................] - ETA: 14:46 - loss: 1.0954 - acc: 0.4188"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4afdacd5bba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing import text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import utils\n",
    "from keras.layers import Dense, Flatten, LSTM, Embedding, Dropout\n",
    "\n",
    "def tokenize_input(train, test, vocab):\n",
    "    # tokenize input text\n",
    "    tokenizer = text.Tokenizer(num_words=vocab)\n",
    "    tokenizer.fit_on_texts(train)\n",
    "    # create input matrix\n",
    "    return tokenizer.texts_to_matrix(train), \\\n",
    "           tokenizer.texts_to_matrix(test), \\\n",
    "           tokenizer\n",
    "\n",
    "def encode_labels(train, test):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(train)\n",
    "    y_train = encoder.transform(train)\n",
    "    num_classes = np.max(y_train) + 1\n",
    "    return utils.to_categorical(y_train, num_classes), \\\n",
    "           utils.to_categorical(encoder.transform(test), num_classes), \\\n",
    "           encoder.classes_, num_classes\n",
    "\n",
    "def split_dataset(dataset, split, input_field, label_field, ):\n",
    "    train_size = int(len(dataset[input_field]) * split)\n",
    "    return dataset[input_field][:train_size], dataset[input_field][train_size:], \\\n",
    "           dataset[label_field][:train_size], dataset[label_field][train_size:]\n",
    "\n",
    "def init_data(data, vocab_size, data_split):\n",
    "    train_input, test_input, train_label, test_label = split_dataset(total_dict, data_split, 'text', 'author')\n",
    "    x_train, x_test, tokenizer = tokenize_input(train_input, test_input, vocab_size)\n",
    "    y_train, y_test, text_labels, num_classes = encode_labels(train_label, test_label)\n",
    "    return train_input, test_input, train_label, test_label, x_train, x_test, y_train, y_test, tokenizer, text_labels,\\\n",
    "           num_classes\n",
    "\n",
    "def conv_x(xval, tokenizer):\n",
    "    ret = tokenizer.texts_to_matrix(xval)\n",
    "    return ret\n",
    "\n",
    "def avgPrediction(predictionsArr):\n",
    "    predictionAvgs = []\n",
    "    average = 0;\n",
    "    for x in range(len(predictionsArr[0])):\n",
    "        for y in range(len(predictionsArr)):\n",
    "            average += predictionsArr[y][x] / len(predictionsArr)\n",
    "        predictionAvgs.append(average)\n",
    "    # print it\n",
    "            \n",
    "\n",
    "train_input, test_input, train_label, test_label, x_train, x_test, y_train, y_test, tokenizer, text_labels, \\\n",
    "    num_classes = init_data(total_df, total_word_count, .7)\n",
    "\n",
    "#pred_x = conv_x(\"Well, if I must, I must,’ the King said, with a melancholy air\", tokenizer)\n",
    "#pred_x = conv_x(\"Well, well, well! Stubb knows him best of all, and Stubb always says he’s queer; says nothing but that one sufficient little word queer; he’s queer, says Stubb; he’s queer—queer, queer; and keeps dinning it into Mr. Starbuck all the time—queer—sir—queer, queer, very queer. And here’s his leg! Yes, now that I think of it, here’s his bedfellow! has a stick of whale’s jaw-bone for a wife! And this is his leg; he’ll stand on this. What was that now about one leg standing in three places, and all three places standing in one hell—how was that? Oh! I don’t wonder he looked so scornful at me! I’m a sort of strange-thoughted sometimes, they say; but that’s only haphazard-like. Then, a short, little old body like me,\", tokenizer)\n",
    "pred_x = conv_x(\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore\", tokenizer)\n",
    "\n",
    "print(x_train.shape)\n",
    "model = Sequential([\n",
    "    Embedding(total_word_count, 32),\n",
    "    LSTM(3),\n",
    "    Dropout(.2),\n",
    "    Dense(512, input_shape=(total_word_count,)),\n",
    "    Dense(3, activation='sigmoid')\n",
    "    #Dense(512),\n",
    "    #Activation('sigmoid'), #TODO: change to sigmoid?\n",
    "    #Dense(num_classes),\n",
    "    #Activation('softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy',optimizer ='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1, validation_split=0.05)\n",
    "\n",
    "pred = model.predict(pred_x)\n",
    "\n",
    "#scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "#print(scores)\n",
    "\n",
    "print(pred)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "#model.fit(x_train,y_train,epochs=2000,batch_size=5,validation_split=0.05,verbose=0);\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
